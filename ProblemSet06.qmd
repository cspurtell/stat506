---
title: "Purtell_ProblemSet06"
author: "Colin Purtell"
format:
  html: 
    embed-resources: true
    standalone: true
editor: visual
---

## STATS 506 Problem Set 06

Link to repository: <https://github.com/cspurtell/stat506>

```{r}
library(Rcpp)
library(e1071)
library(microbenchmark)
library(parallel)
library(dplyr)
library(lme4)
library(purrr)
library(ggplot2)
library(data.table)
```

### Problem 1

Function definition:

```{r}
cppFunction('
double C_moment(NumericVector v, int k) {
  int n = v.size();
  if (n == 0) return NA_REAL;
  
  double sum = 0.0;
  for (int i = 0; i < n; ++i) {
    sum += v[i];
  }
  double mean = sum / n;
  
  double acc = 0.0;
  for (int i = 0; i < n; ++i) {
    double diff = v[i] - mean;
    double term = 1.0;
    for (int j = 0; j < k; ++j) {
      term *= diff;
    }
    
    acc += term;
  }
  
  return acc / n;
}')
```

Testing and comparing to e1071's *moment* function:

```{r}
set.seed(123)
x <- rnorm(100)
k <- 3

C_moment(x, k)
moment(x, order = k, center = TRUE, absolute = FALSE)
```

```{r}
microbenchmark(
  C_moment(x, k),
  moment(x, order = k, center = TRUE, absolute = FALSE),
  times = 50
)
```

### Problem 2

```{r}
source("ProblemSet05.R")
```

#### a.

Class definition:

```{r}
setClass(
  Class = "bootstrapWaldCI",
  contains = "waldCI",
  slots = list(
    fun = "function",
    data = "ANY",
    reps = "integer",
    compute = "character",
    bootStats = "numeric"
  )
)
```

Validity function:

```{r}
setValidity("bootstrapWaldCI", function(object) {
  msgs <- character()
  if (length(object@reps) != 1 || object@reps <= 0) {
    msgs <- c(msgs, "reps must be a single positive integer")
  }
  if (!object@compute %in% c("serial", "parallel")) {
    msgs <- c(msgs, "compute must be either 'serial' or 'parallel'")
  }
  if (length(object@bootStats) != as.integer(object@reps)) {
    msgs <- c(msgs, "length(bootStats) must equal number of reps")
  }
  if (length(msgs) == 0) TRUE else msgs
})
```

Helper function (implemented in constructor and rebootstrap):

```{r}
#' Internal bootstrap helper
#'
#' Runs a nonparametric bootstrap on a statistic function, either in serial
#' or using a simple parallel backend (on Windows via socket clusters).
#'
#' @param FUN A function that takes a data object like a data frame and returns
#'   a numeric scalar statistic.
#' @param data The data to be bootstrapped. Typically a data frame or matrix.
#' @param reps Integer number of bootstrap repetitions.
#' @param compute Character string, either "serial" or \"parallel",
#'   indicating whether to run the bootstrap in serial or using parallel.
#'
#' @return A numeric vector of length reps containing bootstrap
#'   replicate values of the statistic.
.runBootstrap <- function(FUN, data, reps, compute = c("serial", "parallel")) {
  compute <- match.arg(compute)
  FUN     <- match.fun(FUN)
  reps    <- as.integer(reps)
  n <- nrow(data)
  
  one_boot <- function(i) {
    idx <- sample.int(n, n, replace = TRUE)
    
    if (is.null(dim(data))) {
      FUN(data[idx])
    } else {
      FUN(data[idx, , drop = FALSE])
    }
  }
  
  if (compute == "serial") {
    stats <- sapply(seq_len(reps), one_boot)
  } else {
    num_cores <- max(1, detectCores() %/% 2)
    cl <- makeCluster(num_cores)
    on.exit(stopCluster(cl), add = TRUE)
    stats <- parSapply(cl, seq_len(reps), one_boot)
  }
  
  as.numeric(stats)
}
```

Constructor:

```{r}
#' Construct a bootstrap-based Wald confidence interval
#'
#' @description
#' Creates an object of class "bootstrapWaldCI"} by bootstrapping a
#' scalar statistic and then applying the Wald interval formula to the
#' bootstrap mean and standard error.
#'
#' @param FUN A function that takes the data object like a data frame and
#'   returns a numeric scalar statistic.
#' @param data The data to be bootstrapped
#' @param reps Integer number of bootstrap repetitions. Default is 100.
#' @param level Confidence level for the Wald interval. Must be strictly between
#'   0 and 1. Default is 0.95.
#' @param compute Character string, either "serial" or "parallel",
#'   indicating whether the bootstrap should be run in serial or using a
#'   simple parallel backend.
#'
#' @return An object of class "bootstrapWaldCI" that inherits from
#'   "waldCI" and contains the lower and upper confidence bounds,
#'   confidence level, the original statistic function, data, number of
#'   bootstrap repetitions, compute mode, and the bootstrap replicate values.
makeBootstrapCI <- function(FUN,
                            data,
                            reps = 100,
                            level = 0.95,
                            compute = c("serial", "parallel")) {
  compute <- match.arg(compute)
  FUN <- match.fun(FUN)
  reps <- as.integer(reps)
  
  bootStats <- .runBootstrap(FUN, data, reps, compute)
  boot_mean <- mean(bootStats)
  boot_se <- sd(bootStats)
  
  parent_ci <- makeWaldCI(
    level = level,
    mean = boot_mean,
    sterr = boot_se
  )
  
  obj <- new(
    "bootstrapWaldCI",
    lb = lb(parent_ci),
    ub = ub(parent_ci),
    level = level(parent_ci),
    fun = FUN,
    data = data,
    reps = reps,
    compute = compute,
    bootStats = bootStats
  )
  
  validObject(obj)
  obj
}
```

*rebootstrap* function:

```{r}
#' Re-run a bootstrap for a bootstrapWaldCI object
#'
#' @description
#' Recomputes the bootstrap replicates and updates the Wald confidence interval
#' for an existing "bootstrapWaldCI" object, using the same statistic
#' function, data, number of repetitions, confidence level, and compute mode.
#'
#' @param object An object of class "bootstrapWaldCI".
#'
#' @return The same "bootstrapWaldCI" object, with updated lb,
#'   ub, and bootStats slots.
setGeneric("rebootstrap", function(object) standardGeneric("rebootstrap"))
setMethod("rebootstrap", "bootstrapWaldCI", function(object) {
  FUN <- object@fun
  data <- object@data
  reps    <- object@reps
  level   <- object@level
  compute <- object@compute
  
  bootStats <- .runBootstrap(FUN, data, reps, compute)
  boot_mean <- mean(bootStats)
  boot_se   <- sd(bootStats)
  z         <- qnorm((1 + level) / 2)
  half_w    <- z * boot_se
  
  object@lb        <- boot_mean - half_w
  object@ub        <- boot_mean + half_w
  object@bootStats <- bootStats
  
  validObject(object)
  object
})
```

#### b.

```{r}
ci1 <- makeBootstrapCI(function(x) mean(x$y),
                       ggplot2::diamonds,
                       reps = 1000)
ci1
```

```{r}
rebootstrap(ci1)
```

Performance comparison between serial and parallel compute methods:

```{r}
t_serial <- system.time({
  ci_serial <- makeBootstrapCI(function(x) mean(x$y), 
                               ggplot2::diamonds, 
                               reps = 3000, 
                               compute = "serial")
})
t_serial

t_parallel <- system.time({
  ci_parallel <- makeBootstrapCI(function(x) mean(x$y), 
                                 ggplot2::diamonds, 
                                 reps = 3000, 
                                 compute = "parallel")
})
t_parallel
```

#### c.

```{r}
#' Extract the coefficient of disp from a linear model
#'
#' @description
#' Fits the linear model mpg ~ cyl + disp + wt to the supplied data
#' and returns the estimated coefficient corresponding to disp.
#'
#' @param data A data frame containing at least the variables mpg,
#'   cyl, disp, and wt. Typically mtcars.
#'
#' @return A single numeric value giving the estimated coefficient of
#'   disp in the linear model.
dispCoef <- function(data) {
  fit <- lm(mpg ~ cyl + disp + wt, data = data)
  unname(coef(fit)["disp"])
}
```

```{r}
ci2 <- makeBootstrapCI(dispCoef,
                       mtcars,
                       reps = 1000)
ci2
```

```{r}
rebootstrap(ci2)
```

Performance comparison between serial and parallel compute methods:

```{r}
t_serial <- system.time({
  ci_serial <- makeBootstrapCI(dispCoef,
                         mtcars,
                         reps = 3000,
                         compute = "serial")
})
t_serial

t_parallel <- system.time({
  ci_parallel <- makeBootstrapCI(dispCoef,
                               mtcars,
                               reps = 3000,
                               compute = "parallel")
})
t_parallel
```

### Problem 3

```{r}
source("ps06q3.R")
```

#### a.

Model generation:

```{r}
df_std <- df %>%
  group_by(country) %>%
  mutate(
    prior_gpa_z = scale(prior_gpa)[,1],
    forum_posts_z = scale(forum_posts)[,1],
    quiz_attempts_z = scale(quiz_attempts)[,1],
  ) %>% 
  ungroup()

df_by_country <- split(df_std, df_std$country)

#' Fit a mixed-effects logistic regression by country
#'
#' @description
#' Fits a generalized linear mixed-effects model predicting course completion
#' from standardized prior GPA, forum posts, and quiz attempts, with a random
#' intercept for device type.
#'
#' @param data A data frame for a single country, containing the variables
#'   completed_course, prior_gpa_z, forum_posts_z,
#'   quiz_attempts_z, and device_type.
#'
#' @return An object of class "merMod" representing the fitted
#'   mixed-effects logistic regression model.
fit_country_model <- function(data) {
  glmer(
    completed_course ~ prior_gpa_z + forum_posts_z + quiz_attempts_z + (1 | device_type),
    data = data,
    family = binomial,
    control = glmerControl(optimizer = "bobyqa")
  )
}
models <- map(df_by_country, fit_country_model)
```

Plotting the estimates coefficients:

```{r}
coef_df <- map_df(
  names(models),
  ~ {
    m <- models[[.x]]
    tibble(
      country = .x,
      est = fixef(m)["forum_posts_z"],
      se = sqrt(diag(vcov(m)))["forum_posts_z"]
    )
  }
)

coef_df %>%
  mutate(
    lower = est - 1.96 * se,
    upper = est + 1.96 * se
  ) %>%
  ggplot(aes(x = est, y = country)) +
  geom_point(size = 3, color = "steelblue") +
  geom_errorbarh(aes(xmin = lower, xmax = upper), height = 0.2) +
  geom_vline(xintercept = 0, linetype = "dashed") +
  labs(
    title = "Estimated Effect of Forum Posts on Course Completion",
    x = "Coefficient (log-odds scale)",
    y = "Country"
  ) +
  theme_minimal(base_size = 14)
```

Recalculating the models while determining the run times for each:

```{r}
model_times <- data.frame(
  country = names(df_by_country),
  user = NA_real_,
  system = NA_real_,
  elapsed = NA_real_,
  stringsAsFactors = FALSE
)

models <- list()
for (i in seq_along(df_by_country)) {
  country_name <- names(df_by_country)[i]
  dat <- df_by_country[[i]]
  
  t <- system.time({
    m <- fit_country_model(dat)
  })
  
  models[[country_name]] <- m
  model_times$user[i]    <- t[["user.self"]]
  model_times$system[i]  <- t[["sys.self"]]
  model_times$elapsed[i] <- t[["elapsed"]]
}

print(model_times)
```

Insert analysis here

#### b.

```{r}
t_fast <- system.time({
  df_std <- df %>%
    group_by(country) %>%
    mutate(
      prior_gpa_z     = as.numeric(scale(prior_gpa)),
      forum_posts_z   = as.numeric(scale(forum_posts)),
      quiz_attempts_z = as.numeric(scale(quiz_attempts))
    ) %>%
    ungroup()
  
  df_by_country <- split(df_std, df_std$country)
  
  #' Fit mixed model and extract forum post coefficient
  #'
  #' @description
  #' Helper function that fits the mixed-effects logistic regression model for a
  #' single country and extracts the coefficient and standard error for the
  #' standardized forum posts predictor.
  #'
  #' @param dat A data frame for a single country, containing the standardized
  #'   predictors and outcome as in fit_country_model().
  #'
  #' @return A list with elements est (the coefficient estimate) and
  #'   se (its standard error).
  fit_and_extract <- function(dat) {
    m <- glmer(
      completed_course ~ prior_gpa_z + forum_posts_z + quiz_attempts_z +
        (1 | device_type),
      data = dat,
      family = binomial,
      control = glmerControl(optimizer = "bobyqa")
    )
    
    cf   <- fixef(m)
    est  <- unname(cf["forum_posts_z"])
    se   <- sqrt(diag(vcov(m)))["forum_posts_z"]
    
    list(est = est, se = se)
  }
  
  num_cores <- max(1, detectCores() %/% 2)
  cl <- makeCluster(num_cores)
  on.exit(stopCluster(cl), add = TRUE)
  clusterEvalQ(cl, { library(lme4); NULL })
  
  clusterExport(
    cl,
    varlist = c("df_by_country", "fit_and_extract"),
    envir = environment()
  )
  
  countries <- names(df_by_country)
  
  res_list <- parLapply(cl, countries, function(ctry) {
    dat <- df_by_country[[ctry]]
    out <- fit_and_extract(dat)
    data.frame(
      country = ctry,
      est     = out$est,
      se      = out$se,
      stringsAsFactors = FALSE
    )
  })
  
  coef_fast <- do.call(rbind, res_list)
  coef_fast <- coef_fast[order(coef_fast$country), ]
})
```

```{r}
t_fast
```

```{r}
coef_df
```

```{r}
coef_fast
```

We can observe from the above tables that the estimated coefficients using the optimized script are the same as those found in part (a)

### Problem 4

```{r}
atp = fread("data/atp_matches_2019.csv")
```

#### a.

```{r}
tournaments <- unique(atp[, .(tourney_id, tourney_name)])
tournaments[, .N]
```

The ATP dataset includes some tournaments that began in late 2018 but are part of the 2019 competitive season (for example, tournaments starting December 31, 2018). To handle this, I chose to include only tournaments that ended in 2019, reasoning that these were part of the 2019 season. Based on this filter, there were 128 tournaments in the 2019 season.

#### b.

```{r}
tourney_winners <- atp[
  round == "F",
  .(winner = first(winner_name)),
  by = .(tourney_id, tourney_name)
]
```

For consistency, I defined a ‘tournament’ as any event with a recorded final match (round == "F"). This ensures the champion is identifiable. Using this criterion, 67 tournaments had a final recorded in 2019.

```{r}
winner_counts <- tourney_winners[, .N, by = winner][order(-N)]
multi_winners <- winner_counts[N > 1]
multi_winners[]
```

We can see that 12 players won more than one tournament, with the most being 5 by Dominic Thiem and Novak Djokovic.

#### c.

```{r}
aces_long <- melt(
  atp,
  measure.vars  = c("w_ace", "l_ace"),
  variable.name = "result",
  value.name    = "aces"
)
aces_long[, result := ifelse(result == "w_ace", "winner", "loser")]
aces_long <- aces_long[!is.na(aces)]

aces_summary <- aces_long[
  , .(
    mean_aces   = mean(aces),
    median_aces = median(aces),
    sd_aces     = sd(aces),
    n           = .N
  ),
  by = result
]

aces_summary
```

We can see from the observed mean and sd for each group of players that the mean number of aces for winners is higher than for losers.

```{r}
ggplot(aces_long, aes(x = result, y = aces, fill = result)) +
  geom_boxplot() +
  labs(
    title = "Aces by Match Result (Winner vs. Loser)",
    x = "Result",
    y = "Number of Aces"
  ) +
  theme_minimal()
```

The box plots support the hypothesis, as the box and IQR for winners is significantly higher than for losers and with less spread.

#### d.

```{r}
player_stats <- melt(
  atp,
  measure.vars  = c("winner_name", "loser_name"),
  variable.name = "result",
  value.name    = "player"
)
player_stats[, win := as.integer(result == "winner_name")]

player_summary <- player_stats[
  , .(
    matches  = .N,
    wins     = sum(win),
    win_rate = sum(win) / .N
  ),
  by = player
][matches >= 5][order(-win_rate)]

player_summary[win_rate == max(win_rate)]
```

From the above filtering, we find that Rafael Nadal has the highest win rate of around 87%.

#### Resources Used:

R documentation for unname() function and bootstrapping assistance, data.table library documentation for Problem 4 (especially melt), ChatGPT for debugging and roxygen assistance
